{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# train GPT2 not (only) in English\n",
    "\n",
    "mlugs, 2020-09-15  \n",
    "Andreas Madsack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## agenda\n",
    "\n",
    "1. what is GPT2?\n",
    "2. huggingface transformers\n",
    "3. karpathy minGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## what is GPT2\n",
    "\n",
    "- developed by OpenAI\n",
    "- encoder: transformer-based language model\n",
    "- decoder: next word prediction\n",
    "- GPT2 is 10x GPT\n",
    "- GPT3 is 10x GPT2 -- 175 billion parameters\n",
    "- newest version (GPT3) is somehow closed source\n",
    "- GPT3 paper: https://arxiv.org/abs/2005.14165\n",
    "- training-data (of GPT3) >500GB text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### openai code / forks\n",
    "\n",
    "- https://github.com/openai/gpt-2 -- unmaintained, hard to use\n",
    "- https://github.com/nshepperd/gpt-2 -- everyone used this fork\n",
    "- example to train jpop (japanese) GPT2:  \n",
    "  https://medium.com/@ngwaifoong92/beginners-guide-to-retrain-gpt-2-117m-to-generate-custom-text-content-8bb5363d8b7f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## huggingface transformers\n",
    "\n",
    "- https://huggingface.co/\n",
    "- all opensource, good documentation, focus on BERT, but supports GPT2\n",
    "- pytorch and tensorflow!\n",
    "- really good: https://github.com/huggingface/tokenizers - rust+python\n",
    "- ready to use BPE tokenizer - Byte Pair Encoding - best solution for unknown words\n",
    "- because Zipf's law: `The probability of occurrence of words or other items starts high and tapers off. Thus, a few occur very often while many others occur rarely.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f45841bdd00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "text = \"My name is Bob, my favorite\"\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens = torch.tensor([tokens])\n",
    "tokens = model.generate(tokens, max_length=20+tokens.shape[1], do_sample=True)\n",
    "tokens = tokens[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Bob, my favorite and I\\'m on my way.\"\\n\\nThe show began with what the creator told the Daily News'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### pretrained models in huggingface model zoo\n",
    "\n",
    "- gpt2 (small, medium, large)\n",
    "- distilgpt2 (smaller, but nearly same results)\n",
    "- pierreguillou/gpt2-small-portuguese -- https://medium.com/@pierre_guillou/faster-than-training-from-scratch-fine-tuning-the-english-gpt-2-in-any-language-with-hugging-f2ec05c98787\n",
    "- anonymous-german-nlp/german-gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfa/.virtualenv/tmp-f14638b6e96f850/lib/python3.8/site-packages/transformers/modeling_auto.py:819: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer_de = AutoTokenizer.from_pretrained(\"anonymous-german-nlp/german-gpt2\")\n",
    "model_de = AutoModelWithLMHead.from_pretrained(\"anonymous-german-nlp/german-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "text = \"Mein name ist Hans, am liebsten \"\n",
    "tokens = tokenizer_de.encode(text)\n",
    "tokens = torch.tensor([tokens])\n",
    "tokens = model_de.generate(tokens, max_length=20+tokens.shape[1], do_sample=True)\n",
    "tokens = tokens[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mein name ist Hans, am liebsten!!\\nMein Name ist Peter, am liebsten!!\\nIch bin 26 Jahre alt, hellwach'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_de.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Main problem? -> Trainingdata!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### another example using huggingface\n",
    "\n",
    "- https://github.com/mgrankin/ru_transformers\n",
    "- very good description what has to be done\n",
    "- training time: ```Nvidia Titan RTX an epoch takes 70 minutes and the same epoch takes 12.5 minutes on TPU v3-8```\n",
    "- 80+ epochs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## karpathy minGPT\n",
    "\n",
    "- https://github.com/karpathy/minGPT/\n",
    "- pytorch only implementation of GPT2\n",
    "- no BPE support (yet)\n",
    "- char generation is a lot better than \"classical\" RNN generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example for Latin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- copy `mingpt` folder from https://github.com/karpathy/minGPT/ here\n",
    "- download la_dedup.txt.gz from https://oscar-public.huma-num.fr/shuffled/la_dedup.txt.gz\n",
    "- training time about 1 day on RTX-2070."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "from mingpt.utils import sample\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import gzip\n",
    "import math\n",
    "\n",
    "block_size = 200  # spatial extent of the model for its context\n",
    "batch_size = 64  # maximum possible for GPU\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print(\"data has %d characters, %d unique.\" % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / (self.block_size + 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # we're actually going to \"cheat\" and pick a spot in the dataset at random\n",
    "        i = np.random.randint(0, len(self.data) - (self.block_size + 1))\n",
    "        chunk = self.data[i : i + self.block_size + 1]\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 8622770 characters, 2232 unique.\n"
     ]
    }
   ],
   "source": [
    "text = gzip.open(\"la_dedup.txt.gz\", \"r\").read().decode(\"utf-8\")\n",
    "train_dataset = CharDataset(text, block_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "mconf = GPTConfig(\n",
    "    vocab_size=train_dataset.vocab_size,\n",
    "    block_size=train_dataset.block_size,\n",
    "    n_layer=8,n_head=8,n_embd=512,\n",
    ")\n",
    "\n",
    "tconf = TrainerConfig(\n",
    "    max_epochs=200,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=6e-4,\n",
    "    lr_decay=True,\n",
    "    warmup_tokens=batch_size * 20,\n",
    "    final_tokens=200 * len(train_dataset) * block_size,\n",
    "    num_workers=4,\n",
    ")\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_latin = GPT(mconf)\n",
    "model_latin.load_state_dict(torch.load(\"latin_char_model.pt\", map_location=torch.device('cpu')))\n",
    "\n",
    "context = \"Delectus magni accusamus ut qui.\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None, ...]\n",
    "y = sample(model_latin.cpu(), x, 200, temperature=0.9, sample=True, top_k=5)[0]\n",
    "completion = \"\".join([train_dataset.itos[int(i)] for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Delectus magni accusamus ut qui.\\nMinus id quod rem faciendi. Aliquam deleniti dolorem excepturi deserunt saepe ut error consequatur. Eos aut ea non in aliquid. Non numquam eius modi tempora incidunt. Illic quodam facilis et Iustus q'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
